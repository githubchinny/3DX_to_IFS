{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3DX to IFS file\n",
    "\n",
    "This script reads the latest 3dx files and extracts the columns we've configured for IFS migration.  This is a stop-gap while we are still working with extract files and will be replaced by a direct migration of data from 3DX straight into IFS - using IFS connect to drop files for processing on an internal IFS queue.\n",
    "\n",
    "\n",
    "Requirements:    \n",
    "An 3dx extracted csv file in Engineering BoM sharepoint directory for the selected project, eg 'T33-BoM-XP_collated_BOM.csv'\n",
    "\n",
    "\n",
    "Inputs:\n",
    "Project name (T50, T50s, T33_XP...)\n",
    "Previous timestamp \n",
    "\n",
    "Outputs:   \n",
    "Writes compare files, Delta files, and migration txt files to:   \n",
    "\n",
    "Output dir = sharepoint dir, project, IFS   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "\n",
    "import openpyxl\n",
    "import excel_formatting\n",
    "import logging\n",
    "import argparse\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_function_group(df):\n",
    "    # Add Function and Sub Group if it doesn't already exist\n",
    "\n",
    "    # - Level 0 = Model Variant   \n",
    "    # - Level 1 = Function Group Area   \n",
    "    # - Level 2 = System   \n",
    "    # - Level 3 = Sub Systems\n",
    "    # - level 4 = AMs/SAs??\n",
    "\n",
    "    # Find each one and forward fill to the next occurrence\n",
    "    # function group - level 1\n",
    "    df['Function Group'] = np.where(df['Level'].isin([0,1]), df['Description'], None)\n",
    "    df['Function Group'] = df['Function Group'].ffill()\n",
    "\n",
    "    # System - level 2\n",
    "    df['System'] = np.where(df['Level'] == 2, df['Description'], None)\n",
    "    df['System'] = np.where(df['Level'] >= 2, df['System'].ffill(), None)\n",
    "\n",
    "    # SUB_System = level 3\n",
    "    df['Sub System'] = np.where(df['Level'] == 3, df['Description'], None)\n",
    "    df['Sub System'] = np.where(df['Level'] >= 3, df['Sub System'].ffill(), df['Sub System'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parent_part(BOM):\n",
    "    # reset index before trying to update, otherwise multiple rows get updated\n",
    "    BOM.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for sgroup, frame in BOM[BOM['Part Level'] > 2].groupby('Sub Group'):\n",
    "        level = {}\n",
    "\n",
    "        previous_parent_part=0\n",
    "\n",
    "        for i, row in frame.iterrows():\n",
    "            current_part_number = row['Part Number']\n",
    "            current_part_level = row['Part Level']\n",
    "            # reset higher levels for each assembly\n",
    "            if current_part_level == 5:\n",
    "                # remove entries from higher levels\n",
    "                keys = [k for k in level if k > 5]\n",
    "                for x in keys:\n",
    "                    del level[x]\n",
    "\n",
    "            # write part number to dictionary under current part level\n",
    "            level[current_part_level] = current_part_number\n",
    "            # update the current_parent_part if we have current part details (info from catia)\n",
    "            # as we've created level 1 and 2 we don't need this check\n",
    "            # print (current_part_level, current_part_number)\n",
    "            level[2] = group_area_dict[sgroup]\n",
    "            if i > 0:\n",
    "            # get the max part level from the level dictionary that's less than current part level\n",
    "                previous_parent_level = max(k for k in level if k < current_part_level)\n",
    "\n",
    "                    # update the parent part\n",
    "                # print (i, \"Parent part {} from previous level {}\".format(level[previous_parent_level], previous_parent_level))\n",
    "                BOM.at[i,'Parent Part'] = level[previous_parent_level]\n",
    "    return BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_pool_connection(env):\n",
    "    # connection to database using sqlalchemy\n",
    "    import oracledb\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import text\n",
    "    import db_config\n",
    "    import pandas as pd\n",
    "\n",
    "    # d = r\"C:\\Users\\mark.chinnock\\oracle\\instantclient_21_10\"\n",
    "    # oracledb.init_oracle_client(lib_dir=d)\n",
    "\n",
    "    if env == 'PREPROD':\n",
    "        pool = oracledb.create_pool(user=db_config.user, password=db_config.PREPROD_userpwd, dsn=db_config.PREPROD_connect_string,\n",
    "                                min=1, max=5, increment=1)\n",
    "    elif env == 'LIVE':\n",
    "        pool = oracledb.create_pool(user=db_config.user, password=db_config.LIVE_userpwd, dsn=db_config.LIVE_connect_string,\n",
    "                                min=1, max=5, increment=1)\n",
    "    elif env == 'Sandbox':\n",
    "        pool = oracledb.create_pool(user=db_config.user, password=db_config.Sandbox_userpwd, dsn=db_config.Sandbox_connect_string,\n",
    "                                min=1, max=5, increment=1)\n",
    "\n",
    "\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ifs_part_cat(env):\n",
    "    # connection to database using sqlalchemy\n",
    "    import oracledb\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy import text\n",
    "    import db_config\n",
    "    import pandas as pd\n",
    "    from contextlib import suppress\n",
    "\n",
    "    # Database Credentials\n",
    "    username = db_config.user\n",
    "    password = db_config.LIVE_userpwd\n",
    "\n",
    "    engine = create_engine(\n",
    "        f'oracle+oracledb://:@',\n",
    "            thick_mode=None,\n",
    "            connect_args={\n",
    "                \"user\": db_config.user,\n",
    "                \"password\": db_config.LIVE_userpwd,\n",
    "                \"host\": db_config.LIVE_host,\n",
    "                \"port\": 1521,\n",
    "                \"service_name\": db_config.LIVE_service\n",
    "        })\n",
    "\n",
    "    query = (\"select distinct c.part_no, c.unit_meas, p.lot_tracking_code, p.serial_tracking_code, p.serial_rule \"\n",
    "            \"from ifsapp.inventory_part c \"\n",
    "            \"left join ifsapp.part_catalog p \"\n",
    "            \"on c.part_no = p.part_no \"\n",
    "            \"where c.part_no like 'T%'\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # print(connection.scalar(text(\"\"\"SELECT * from IFSAPP.purchase_order_line_all\"\"\")))\n",
    "            query = connection.execute(text(query))\n",
    "\n",
    "        df = pd.DataFrame(query.fetchall())\n",
    "\n",
    "        df.columns = df.columns.str.upper()\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_area_dict(project):\n",
    "    group_area_dict = {\n",
    "        'A02-Panels & Closure Systems':project +'-01', \n",
    "        'A03-Exterior Systems':project +'-01',\n",
    "        'A01-Structure Systems':project +'-01', \n",
    "        'B01-Suspension Systems':project +'-02',\n",
    "        'C01-Braking Systems':project +'-02', \n",
    "        'D01-Steering Systems':project +'-02', \n",
    "        'E01-Pedal System':project +'-02',\n",
    "        'M01-Control Systems':project +'-03', \n",
    "        'M02-Traction Systems':project +'-03',\n",
    "        'M03-Electrical Distribution Sys':project +'-03', \n",
    "        'M04-Multimedia Systems':project +'-03',\n",
    "        'M05-Safety & Security Systems':project +'-03', \n",
    "        'M06-Software Systems':project +'-03',\n",
    "        'N01-Interior & Trim Systems':project +'-04', \n",
    "        'N02-HVAC Systems':project +'-04',\n",
    "        'F01-ICE Powertrain Systems':project +'-03', \n",
    "        'G01-Transmission Systems':project +'-03',\n",
    "        'J01-Pwt NVH & Heatshield Sys':project +'-03', \n",
    "        'L01-Cooling Systems':project +'-03',\n",
    "        'R01-Styling':project +'-07',\n",
    "        'P01-Packaging':project +'-06',\n",
    "        'T01-Tooling':project +'-08',\n",
    "        'U01-Development':project +'-09',\n",
    "        'V01-Accessories':project +'-10'\n",
    "    }\n",
    "\n",
    "    return group_area_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_part_type_dict():\n",
    "    part_type_dict = {\n",
    "    'AIH':'Manufactured',\n",
    "    'BOF':'Purchased',\n",
    "    'BOP':'Purchased',\n",
    "    'CON':'Purchased (Raw)',\n",
    "    'ENG':'Purchased (Raw)',\n",
    "    'FAS':'Purchased (Raw)',\n",
    "    'FIP':'Purchased (Raw)',\n",
    "    'RAW':'Purchased (Raw)',\n",
    "    'MIH':'Manufactured',\n",
    "    'POA':'Manufactured',\n",
    "    'MOB':'Manufactured'\n",
    "    }\n",
    "\n",
    "    return part_type_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sharepoint_dir(project):\n",
    "    import glob\n",
    "    # personal one drive\n",
    "    user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "    if project in (['T33_XP','T50s']):\n",
    "        company = 'gmd'\n",
    "    else:\n",
    "        company = 'gmt'\n",
    "        CMO = True\n",
    "\n",
    "    # replace USERNAME with current logged on user\n",
    "    user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "    # find out what gordonmurray folder to use from the user_dict\n",
    "\n",
    "    # read in config file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('user_directory.ini')\n",
    "\n",
    "    # read in gm_dir and gm_docs from config file\n",
    "    gm_dir = config[os.getlogin().lower()]['gm_dir']\n",
    "    gm_docs = config[os.getlogin().lower()][company]\n",
    "\n",
    "    # go find the Engineering BoM directory within the User directory\n",
    "    # from glob import glob\n",
    "    sharepoint = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "\n",
    "    return (sharepoint, company, CMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_existing_bom(base, bom_file):\n",
    "    path = os.path.join(base, bom_file)\n",
    "    with open(path, \"rb\") as f:\n",
    "        existing_bom = pd.read_excel(f, na_values='*', parse_dates=True) \n",
    "        # sheetnames = [sheet for sheet in f.sheet_names]\n",
    "\n",
    "    return existing_bom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_not_set_values(df):\n",
    "    df = df.replace('Not Set', np.NaN)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_part(df):\n",
    "    # reset index before trying to update, otherwise multiple rows get updated\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df['Parent Part'] = None\n",
    "\n",
    "    level = {}\n",
    "    previous_parent_part=0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        current_part_number = row['Title']\n",
    "        current_part_level = row['Level']\n",
    "\n",
    "        # write part number to dictionary under current part level\n",
    "        level[current_part_level] = current_part_number\n",
    "\n",
    "        # reset higher levels for each assembly\n",
    "        # remove entries from higher levels\n",
    "        keys = [k for k in level if k > current_part_level]\n",
    "        for x in keys:\n",
    "            del level[x]\n",
    "\n",
    "        if current_part_level > 0:\n",
    "            # get the max part level from the level dictionary that's less than current part level\n",
    "            previous_parent_level = max(k for k in level if k < current_part_level)\n",
    "\n",
    "            # update the parent part\n",
    "            # print (i, \"Parent part {} from previous level {}\".format(level[previous_parent_level], previous_parent_level))\n",
    "            df.at[i,'Parent Part'] = level[previous_parent_level]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sa_index(df):\n",
    "    # leave this as int and you get the index with .0 at the end, which helps match row 5, but not row 50, 500, etc\n",
    "    df['SA_Index'] = np.where(df['Level'] == 4, df['orig_sort'].astype(str), np.nan)\n",
    "    df['SA_Index'] = np.where(df['Level'] < 4, df['orig_sort'].astype(str), df['SA_Index'])\n",
    "    # forward fill so that > Level 5 get the same index\n",
    "    df['SA_Index'] = df['SA_Index'].ffill()\n",
    "    # don't include Part level in SA_Index\n",
    "    # cleansed_df['SA_Index'] = cleansed_df['SA_Index'].astype(str) + '_' + cleansed_df['Part Level'].astype(str) + '_' + cleansed_df['Part Number']\n",
    "    # df['SA_Index'] = df['SA_Index'].astype(str) + '_' + df['Title']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sa_index2(df):\n",
    "    df['SA_Index'] = np.where(df['Assembly'], df['orig_sort'].astype(str), np.nan)\n",
    "    df['SA_Index'] = df['SA_Index'].ffill()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_FIPS(df):\n",
    "    df['BOF_Parent'] = np.where(df['Source Code'] == 'BOF', df['Title'], np.nan)\n",
    "    df['BOF_Parent'].ffill(inplace=True)\n",
    "    df['Parent Part'] = np.where(df['Source Code'] == 'FIP', df['BOF_Parent'], df['Parent Part'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prev_mig_files(outdir, prev_timestamps):\n",
    "    import glob\n",
    "    # /**/ makes this recursive through folders in the project specfied\n",
    "    previous_struct_file = glob.glob(outdir + '/**/Structure_*' + prev_timestamps + '.txt', recursive = True)[0]\n",
    "    previous_part_file = glob.glob(outdir + '/**/Part*' + prev_timestamps + '.txt', recursive = True)[0]\n",
    "\n",
    "    # use dtype in read_csv to capture trailing zeros in ENG_PART_REV\n",
    "    path = os.path.join(base, 'IFS', previous_part_file)\n",
    "    with open(path, \"rb\") as f:\n",
    "        prev_part = pd.read_csv(f, sep='\\t', dtype={'ENG_PART_REV':str}) \n",
    "    path = os.path.join(base, 'IFS', previous_struct_file)\n",
    "    with open(path, \"rb\") as f:\n",
    "        prev_struct = pd.read_csv(f, sep='\\t', dtype={'SUB_PART_REV':str}) \n",
    "        # sheetnames = [sheet for sheet in f.sheet_names]\n",
    "\n",
    "    return prev_part, prev_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structure_parts(cleansed_df):\n",
    "# only create structure_df with 4 cols, using Revision once - otherwise we can't rename the columns without it renaming both Revision cols\n",
    "# we'll add the PART_REV later by mapping to the parent_part_dict\n",
    "\n",
    "    structure_cols = [\n",
    "    'Parent Part',\n",
    "    'Revision',\n",
    "    'Title',\n",
    "    'Quantity',\n",
    "    ]\n",
    "\n",
    "    # bring in Release Status from BoM as well\n",
    "    part_cols = [\n",
    "    'Title',\n",
    "    'Level',\n",
    "    'Description',\n",
    "    'Weight',\n",
    "    'Source Code',\n",
    "    'Revision',\n",
    "    'Function Group',\n",
    "    'Sub System',\n",
    "    'Owner',\n",
    "    'Maturity State',\n",
    "    'UOM',\n",
    "    'Provide',\n",
    "    'Inventory Part Planning',\n",
    "    # bring in Phase to be mapped to DEVELOPMENT PHASE\n",
    "    'Phase'\n",
    "    ]\n",
    "\n",
    "    # don't need to bring the first row from cleansed_df into the structure file.\n",
    "    structure_df = cleansed_df[1:][structure_cols].copy()\n",
    "    parts_df = cleansed_df[part_cols].copy()\n",
    "\n",
    "    # rename cols to match IFS naming\n",
    "    structure_df.rename(columns={\n",
    "        'Parent Part':'PART_NO', \n",
    "        'Title':'SUB_PART_NO',\n",
    "        'Quantity':'QTY',\n",
    "        'Revision':'SUB_PART_REV'   \n",
    "    }, inplace=True)\n",
    "\n",
    "    parts_df.rename(columns={\n",
    "        'Title':'PART_NO',\n",
    "        'Description':'DESCRIPTION',\n",
    "        'Weight':'WEIGHT_NET',\n",
    "        'Function Group':'FUNCTION_GROUP',\n",
    "        'Sub System':'SUB_GROUP',\n",
    "        'Level':'PART_LEVEL',\n",
    "        'Owner':'PART_RESPONSIBLE',\n",
    "        'Source Code':'SOURCE_CODE',\n",
    "        'Maturity State':'RELEASE_STATUS',\n",
    "        'UOM':'UNIT_CODE',\n",
    "        'Provide':'PROVIDE',\n",
    "        'Inventory Part Planning':'INVENTORY_PART_PLANNING',\n",
    "        # mapping Phase to MATURITY until Ryan made his changes\n",
    "        'Phase':'MATURITY'\n",
    "        }\n",
    "        , inplace=True)\n",
    "\n",
    "\n",
    "    return structure_df, parts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_IFS(df):\n",
    "\n",
    "    cols = ['orig_sort',\n",
    "    'Function Group',\n",
    "    'System',\n",
    "    'Sub System', \n",
    "    'Level',\n",
    "    'Title', \n",
    "    'Revision',\n",
    "    'Description', \n",
    "    'Quantity',\n",
    "    'Source Code',\n",
    "    'UOM',\n",
    "    'Provide',\n",
    "    'Actual Mass',\n",
    "    'CAD Mass',\n",
    "    'CAD Maturity',\n",
    "    # 'Change Type',\n",
    "    'Maturity State',\n",
    "    'Inventory Part Planning',\n",
    "    'Name',\n",
    "    'Owner',\n",
    "    # Part Type from 3dx is different to what IFS Part type is\n",
    "    # 'Part Type',\n",
    "    # 'Piece Cost',\n",
    "    # 'Piece Cost Estimated',\n",
    "    # 'Piece Cost Maturity',\n",
    "    # 'Service Part Flag',\n",
    "    # 'Tooling Cost Target',\n",
    "    # 'Tooling Cost Estimated',\n",
    "    # 'Assembly',\n",
    "    # 'Part',\n",
    "    'Phase'\n",
    "    ]\n",
    "\n",
    "    # merged = pd.merge(df, existing_bom[cols], on='Part Number', how='left', indicator=True)\n",
    "    IFS = df[cols]\n",
    "\n",
    "    return IFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_delta_files(parts_df, structure_df):\n",
    "    # build a dictionary from a copy\n",
    "    dict_part_compare = {1:prev_part.copy(),2:parts_df.copy()}\n",
    "    dict_struct_compare = {1:prev_struct.copy(),2:structure_df.copy()}\n",
    "\n",
    "    dict_part_compare[1]['ENG_PART_REV'] = dict_part_compare[1]['ENG_PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_part_compare[2]['ENG_PART_REV'] = dict_part_compare[2]['ENG_PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[1]['PART_REV'] = dict_struct_compare[1]['PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[2]['PART_REV'] = dict_struct_compare[2]['PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[1]['SUB_PART_REV'] = dict_struct_compare[1]['SUB_PART_REV'].astype(str).str.split('.').str[0]\n",
    "    dict_struct_compare[2]['SUB_PART_REV'] = dict_struct_compare[2]['SUB_PART_REV'].astype(str).str.split('.').str[0]    \n",
    "\n",
    "    dict_struct_compare2=pd.concat(dict_struct_compare)\n",
    "    dict_part_compare2=pd.concat(dict_part_compare)\n",
    "\n",
    "    # ignore_cols = ['WEIGHT_NET']\n",
    "    dict_part_compare2.WEIGHT_NET = np.round(dict_part_compare2.WEIGHT_NET,4).astype(str)\n",
    "\n",
    "    subset_cols = []\n",
    "    if ignore_cols_for_comparison is not None:\n",
    "        print (\"cols being ignored {}\".format(ignore_cols_for_comparison))\n",
    "        subset_cols = dict_part_compare2.drop(columns=ignore_cols_for_comparison).columns\n",
    "        # delta_parts and delta_struct have the rows with changes\n",
    "        delta_parts = dict_part_compare2[dict_part_compare2['PART_LEVEL']>=4].drop_duplicates(subset=subset_cols, keep=False)\n",
    "\n",
    "    else:\n",
    "        # print (\"no cols to ignore\")\n",
    "        delta_parts = dict_part_compare2[dict_part_compare2['PART_LEVEL']>=4].drop_duplicates(keep=False)\n",
    "\n",
    "    delta_struct = dict_struct_compare2.drop_duplicates(keep=False)\n",
    "\n",
    "    return delta_parts, delta_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Part Level'].shift(-1) <= df['Part Level'])].SA_Index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    return make_no_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_diff(data, color='pink'):\n",
    "    # Define html attribute\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    other = data.xs('Previous', axis='columns', level=-1)\n",
    "    # Where data != other set attribute\n",
    "    return pd.DataFrame(np.where((data.ne(other, level=0)), attr, ''),\n",
    "                        index=data.index, columns=data.columns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parts(df):\n",
    "    try:\n",
    "        df_all = pd.concat([df.loc[1,].set_index('PART_NO'), df.loc[2,].set_index('PART_NO')], axis='columns', keys=['Previous','Current'])\n",
    "        df_final = df_all.swaplevel(axis='columns')[delta_parts.columns[1:]].fillna('')\n",
    "    except:\n",
    "        df_all = pd.concat([pd.DataFrame('', columns=delta_parts.columns, index=delta_parts['PART_NO']), delta_parts.loc[2,].set_index('PART_NO')], axis='columns', keys=['Previous','Current'])\n",
    "        df_final = df_all.swaplevel(axis='columns')[delta_parts.columns[1:]].fillna('')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_struct(df):\n",
    "    indx = ['PART_NO','SUB_PART_NO']\n",
    "    df_all = pd.concat([df.loc[1,].set_index(indx), df.loc[2,].set_index(indx)], axis='columns', keys=['Previous','Current'])\n",
    "    df_final = df_all.swaplevel(axis='columns')[df.drop(columns=indx).columns].fillna('')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_source_codes(df):\n",
    "    # find blank source codes and drop the whole assembly\n",
    "    empty_sc = set(df['SA_Index'][(df['Source Code'].isna()) & (df['Level'] > 3)])\n",
    "\n",
    "    # remove any assemblies with empty source codes\n",
    "    if len(empty_sc) > 0:\n",
    "        pat = '|'.join(r\"\\b^{}\\b\".format(x) for x in empty_sc)\n",
    "        sc_df = df[~df['SA_Index'].str.contains(pat)]\n",
    "\n",
    "    else:\n",
    "        logging.info(\"There are no blank source codes to worry about\")\n",
    "        sc_df = df\n",
    "\n",
    "    return sc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_source_codes2(df):\n",
    "    # attempting to drop rows for this part and any child part, but you need to iterate to the bottom of the assembly.\n",
    "    # might as well drop the whole assembly\n",
    "    empty_sc = set(df['Title'][(df['Source Code'].isna()) & (df['Level'] > 3)])\n",
    "\n",
    "    # remove any row with a 'Title' or 'Parent Part' that matches empty_sc.\n",
    "    if len(empty_sc) > 0:\n",
    "        pat = '|'.join(r\"\\b^{}\\b\".format(x) for x in empty_sc)\n",
    "        sc_df = df[~df['Title'].str.contains(pat, na=False)]\n",
    "        sc_df = sc_df[~sc_df['Parent Part'].str.contains(pat, na=False)]\n",
    "\n",
    "\n",
    "    else:\n",
    "        logging.info(\"There are no blank source codes to worry about\")\n",
    "        sc_df = df\n",
    "\n",
    "    return sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unreleased_assy(df):\n",
    "    # find unreleased parts/assemblies.  We can't pass any assembly to IFS that isn't completely released.\n",
    "    # must check there is something in the sa_set\n",
    "\n",
    "    # set to give us a unique list\n",
    "    sa_set = set(df['SA_Index'])\n",
    "\n",
    "    if len(sa_set) > 0:\n",
    "        unrel_sa_set = set(df['SA_Index'][df['Maturity State'] != 'Released'])\n",
    "\n",
    "    # get the fully released assemblies by ignoring the ones containing unreleased sa_index\n",
    "    # must check there is something in the unrel_sa_set\n",
    "    if len(unrel_sa_set) > 0:\n",
    "        pat = '|'.join(r\"\\b^{}\\b\".format(x) for x in unrel_sa_set)\n",
    "        rel_df = df[~df['SA_Index'].str.contains(pat)]\n",
    "        # get the unrel rows for writing out the warning messages\n",
    "        unrel_df = df[df['SA_Index'].str.contains(pat)]\n",
    "    else:\n",
    "        # there are no non released sa to worry about.  This will return an empty df\n",
    "        logit.info(\"There are no unreleased parts or assemblies to worry about\")\n",
    "        rel_df = df\n",
    "\n",
    "    return rel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_CMO(df):\n",
    "    # forseven / GMT are not going to manufacture any parts themselves but send to a customer\n",
    "    # therefore, there are no buy assemblies, but these should be phantoms to allow IFS to send parts to an external customer\n",
    "    # input: pass in the parts file \n",
    "    # action: change all level 4 (assemblies) to 'Phantom' - leave everything else the same\n",
    "    # output: return the parts file with the new Provide value of 'Phantom'\n",
    "\n",
    "    df['Provide'] = np.where(df['Level'] == 4, 'Phantom', df['Provide'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    import time\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "    return timestr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore cols: None\n",
      "project: T53\n",
      "bom file: Updated_T53-Z00002_2024-02-20.xlsx\n",
      "env: LIVE\n",
      "company: gmt\n",
      "CMO: True\n"
     ]
    }
   ],
   "source": [
    "if type_of_script() == 'terminal':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"project\", metavar='Project', type=str, help=\"T50, T50s, T33_XP, etc\") \n",
    "    parser.add_argument(\"bom_file\", metavar=\"BoM File\", type=str, help=\"Updated_T48e-01-Z00001_2023-08-18.xlsx\")\n",
    "    parser.add_argument(\"timestamps\", metavar='prev_timestamps', type=str, help='timestamp portion from previous bom file: eg 20230530-2044')\n",
    "    parser.add_argument(\"env\", metavar='Environment', type=str, help='LIVE, PREPROD or Sandbox')\n",
    "    # parser.add_argument(\"incrementer\", metavar='Incrementer', type=int)    \n",
    "    # parser.add_argument('-d', help='Produce Delta files', action='store_true')\n",
    "    parser.add_argument('-i', '--ignore', action='append', help=\"Cols to Ignore from comparison - only needed when we've added/removed a column from migration files\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    project = args.project\n",
    "    bom_file = args.bom_file\n",
    "    prev_timestamps = args.timestamps\n",
    "    env=args.env\n",
    "    ignore_cols_for_comparison = args.ignore\n",
    "    # incrementer=args.incrementer\n",
    "\n",
    "else:\n",
    "    # set defaults if we're running in jupyter\n",
    "    project = 'T53'\n",
    "    # for producing the delta files\n",
    "    bom_file = \"Updated_T53-Z00002_2024-02-20.xlsx\"\n",
    "    prev_timestamps = 'None'\n",
    "    env = 'LIVE'\n",
    "    ignore_cols_for_comparison = None\n",
    "    # incrementer=6\n",
    "    # env = 'Sandbox'\n",
    "\n",
    "  \n",
    "print (\"ignore cols: {}\".format(ignore_cols_for_comparison))\n",
    "print (\"project: {}\".format(project))\n",
    "print (\"bom file: {}\".format(bom_file))\n",
    "print (\"env: {}\".format(env))\n",
    "\n",
    "sharepoint_dir, company, CMO = find_sharepoint_dir(project)\n",
    "\n",
    "print (\"company: {}\".format(company))\n",
    "print (\"CMO: {}\".format(CMO))\n",
    "\n",
    "base = os.path.join(sharepoint_dir, project)\n",
    "# base = user_dir\n",
    "\n",
    "# where we'll write out the files\n",
    "outdir = os.path.join(base, 'IFS')\n",
    "\n",
    "timestr = get_current_time()\n",
    "# logfile name and location\n",
    "logfile = os.path.join(base, 'IFS', 'logs', \"BOM_to_IFS_{}_{}_log.txt\".format(project, timestr))\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.basicConfig(filename=logfile, filemode='a', level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "logit = logging.getLogger(__name__)\n",
    "\n",
    "logit.info(\"Starting the process...\")\n",
    "logit.info(\"Creating files for project {}\".format(project))\n",
    "logit.info(\"Running from {}\".format(type_of_script()))\n",
    "logit.info(\"base: {}\".format(base))\n",
    "logit.info(\"outdir: {}\".format(outdir))\n",
    "logit.info(\"Columns being ignore in comparison later (new cols?): {}\".format(ignore_cols_for_comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open exsting bom\n",
    "existing_bom = open_existing_bom(base, bom_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace underscore in revision\n",
    "Secondary Underscore revisioning was introduced into 3dx on 07/07/2023 and any pat created before this date will get an underscore revision format if it is uprevised/released.  Any part created afterwards will get the normal revision format.  This is a p in the a.  IFS will not accept underscore in the revision field.\n",
    "\n",
    "Changing underscores to '.' - so revision 1_2.1 will become 1.1.1  This will work until we reach 7 characters (including the dots) as IFS can only handle 6 chars\n",
    "\n",
    "Since 3dx migration to GMT tenant, underscores are removed from Revision.  Need to make revision field an object otherwise defaults to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_bom['Revision'] = existing_bom['Revision'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_bom['Revision'] = existing_bom['Revision'].str.replace('_', '.', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first row of percentage missing values\n",
    "existing_bom = existing_bom.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear 'not set' values as they are blank/not completed cells\n",
    "existing_bom = clear_not_set_values(existing_bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_32544\\1709332845.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ifs_parts_cat = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "# call IFS for latest parts catalogue info\n",
    "\n",
    "# get db pool connection\n",
    "pool = db_pool_connection(env)\n",
    "\n",
    "query = (\"select distinct c.part_no, c.unit_meas, p.lot_tracking_code, p.serial_tracking_code, p.serial_rule \"\n",
    "        \"from ifsapp.inventory_part c \"\n",
    "        \"left join ifsapp.part_catalog p \"\n",
    "        \"on c.part_no = p.part_no \"\n",
    "        \"where c.part_no like 'T%'\")\n",
    "\n",
    "with pool.acquire() as connection:\n",
    "    ifs_parts_cat = pd.read_sql(query, connection)\n",
    "\n",
    "\n",
    "\n",
    "# ifs_parts_cat = get_ifs_part_cat(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts_cat_dict for looking up serial tracking setting\n",
    "parts_cat_dict = {}\t\n",
    "parts_cat_dict = pd.Series(ifs_parts_cat['LOT_TRACKING_CODE'].values,index=ifs_parts_cat['PART_NO']).to_dict()\n",
    "\n",
    "ifs_parts_dict = {}\n",
    "ifs_parts_dict = pd.Series(ifs_parts_cat['UNIT_MEAS'].values,index=ifs_parts_cat['PART_NO']).to_dict()    \n",
    "\n",
    "ifs_serial_tracking = {}\n",
    "ifs_serial_tracking = pd.Series(ifs_parts_cat['SERIAL_TRACKING_CODE'].values,index=ifs_parts_cat['PART_NO']).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PART_NO</th>\n",
       "      <th>UNIT_MEAS</th>\n",
       "      <th>LOT_TRACKING_CODE</th>\n",
       "      <th>SERIAL_TRACKING_CODE</th>\n",
       "      <th>SERIAL_RULE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>TFF-AB273</td>\n",
       "      <td>pcs</td>\n",
       "      <td>Not Lot Tracking</td>\n",
       "      <td>Not Serial Tracking</td>\n",
       "      <td>Manual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PART_NO UNIT_MEAS LOT_TRACKING_CODE SERIAL_TRACKING_CODE SERIAL_RULE\n",
       "4194  TFF-AB273       pcs  Not Lot Tracking  Not Serial Tracking      Manual"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ifs_parts_cat[ifs_parts_cat['PART_NO'] == 'TFF-AB273']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove orig_sort if it's there\n",
    "try:\n",
    "    existing_bom.drop('orig_sort', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "existing_bom.reset_index(inplace=True)\n",
    "existing_bom.rename(columns={'index':'orig_sort'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFS = create_IFS(existing_bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_32544\\1918780858.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  IFS.loc[:,'Part Type'] = IFS['Source Code'].map(part_type_dict)\n"
     ]
    }
   ],
   "source": [
    "# have dropped part_type from 3dx and deriving settings for IFS from source code value\n",
    "part_type_dict = create_part_type_dict()\n",
    "IFS.loc[:,'Part Type'] = IFS['Source Code'].map(part_type_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop PACKAGING Function Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop PACKAGING Function Group - all sorts happens in here and it is not required in ERP\n",
    "IFS = IFS[~IFS['Function Group'].str.contains('PACKAGING')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked = IFS.groupby(['Title','Revision']).size().unstack()\n",
    "\n",
    "# find number of columns dynamically, as number of unique status controls the number of columns\n",
    "expected_revision_count = len(unstacked.columns) - 1\n",
    "\n",
    "unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_revision_count].reset_index()\n",
    "# dup_parts = unstacked2['Part Number'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Revision</th>\n",
       "      <th>Title</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>2.0</th>\n",
       "      <th>2.1</th>\n",
       "      <th>3.0</th>\n",
       "      <th>3.1</th>\n",
       "      <th>4.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T53-P00637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T53-P01703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T53-P01705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T53-P02651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Revision       Title  1.0  1.1  1.2  2.0  2.1  3.0  3.1  4.0  6.0  7.0  8.1\n",
       "0         T53-P00637  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  2.0  NaN  1.0\n",
       "1         T53-P01703  NaN  NaN  NaN  4.0  NaN  1.0  NaN  NaN  NaN  NaN  NaN\n",
       "2         T53-P01705  NaN  NaN  NaN  4.0  NaN  1.0  NaN  NaN  NaN  NaN  NaN\n",
       "3         T53-P02651  1.0  NaN  NaN  NaN  NaN  NaN  1.0  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where are they parts with more than one revision\n",
    "unstacked2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Structures\n",
    "\n",
    "Lorena says if empty structure for 'BOF,'BOP', then it should be purchased (Raw) rather than purchased\n",
    "\n",
    "Empty structure = following row has part level <= current part level\n",
    "\n",
    "*** Even though this logic works, we don't supply Part Type to the migration process and Migration itself does this empty structure logic instead ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if BOF or BOP and next row's part level <= current part level update to Purchased (Raw)\n",
    "#shift(-1) reads the next row\n",
    "IFS['Part Type'] = np.where(IFS['Source Code'].isin(['BOF','BOP']) & (IFS['Level'].shift(-1) <= IFS['Level']), 'Purchased (Raw)', IFS['Part Type'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Source Codes\n",
    "\n",
    "03/03/2023: we are going to drop POA rows completely from the files and then correct the parent part to maintain the structure   \n",
    "08/03/2023: we are dropping ENG rows completely from the dataframe and correct the parent part, if needed   \n",
    "09/03/2023: we are dropping SOP rows as well now   \n",
    "21/07/2023: we want to include SOP rows where there is a Service Identifier of 'Y' or 'C' and it is configured for a PROD vehicle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop source codes\n",
    "# IFS = IFS[~IFS['Source Code'].isin(['POA','ENG'])]\n",
    "IFS = IFS[~IFS['Source Code'].isin(['POA','ENG','SOP'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title (Part Number) cannot contain lowercase in IFS\n",
    "IFS['Title'] = IFS['Title'].str.upper()\n",
    "\n",
    "IFS_pp = create_parent_part(IFS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct FIPs \n",
    "\n",
    "FIP parent part needs to be the previous BOF - not sure if this is still valid for GMT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_df = correct_FIPS(IFS_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_df['Weight'] = np.where(cleansed_df['Actual Mass'] > 0, cleansed_df['Actual Mass'], np.NaN)\n",
    "cleansed_df['Weight'] = np.where((cleansed_df['Weight'].isna()) & (cleansed_df['CAD Mass'] > 0), cleansed_df['CAD Mass'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SA Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_sa_index2 is using the assembly flag from 3dx\n",
    "cleansed_df = create_sa_index(cleansed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply business rules\n",
    "\n",
    "Drop unreleased assy   \n",
    "Drop assy with make parts and no buy child parts   \n",
    "Drop assy with missing mandatory data   \n",
    "Drop part numbers > 25 chars long\n",
    "Add released FIPS in unreleased assy to parts delta file   \n",
    "Change Provide to Phantom if we are using a CMO for manufacturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_UOM(df):\n",
    "    # # fill blank UNIT_CODE with LTR for FLA\n",
    "    df.loc[:,'UOM'] = np.where(((df['UOM'].isna()) & (df['Source Code'] == 'FLA')), 'LTR', df['UOM'])\n",
    "\n",
    "    # fill remaining blank UNIT_CODE with PCS\n",
    "    df.loc[:,'UOM'] = df['UOM'].fillna('PCS')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_Source_Code(df):\n",
    "    # fill TFF parts with 'FAS' and populate Provide with 'Buy'\n",
    "    # fill TPP parts with 'BOP' and populate Provide with 'Buy'\n",
    "    df.loc[:,'Source Code'] = np.where(df['Title'].str.contains('^TFF', regex=True, na=False), 'FAS', df['Source Code'])\n",
    "    df.loc[:,'Source Code'] = np.where(df['Title'].str.contains('^TPP', regex=True, na=False), 'BOP', df['Source Code'])\n",
    "    df.loc[:,'Provide'] = np.where(df['Source Code'].isin(['FAS','BOP']), 'Buy', df['Provide'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_mandatory_fields(df):\n",
    "    # just in case these fields haven't been populated in 3dx\n",
    "    df.loc[:,'Provide'] = np.where((df['Provide'].isna() ) & (df['Source Code'].isin(['AIH','MIH','MOB'])),'Make','Buy')\n",
    "    df.loc[:,'Inventory Part Planning'] = np.where((df['Inventory Part Planning'].isna() ) & ( df['Provide'] == 'Make'), 'P', 'A')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_part_numbers(df, length):\n",
    "    long_pn = df[['Title','SA_Index']][df['Title'].str.len() > length]\n",
    "\n",
    "    # remove any assemblies with long part numbers\n",
    "    if len(long_pn) > 0:\n",
    "        pat = '|'.join(r\"\\b^{}\\b\".format(x) for x in long_pn['SA_Index'])\n",
    "        df_removed = df[~df['SA_Index'].str.contains(pat)]\n",
    "\n",
    "        for x, row in long_pn.iterrows():\n",
    "            logging.error(\"Part Number greater than {} chars: {}\".format(length, row.iloc[0]))\n",
    "\n",
    "    else:\n",
    "        # nothing to remove - return the original df\n",
    "        df_removed = df\n",
    "    return (df_removed, long_pn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capitalise Source Code\n",
    "cleansed_df.loc[:,'Source Code'] = cleansed_df['Source Code'].str.upper()\n",
    "\n",
    "# for testing, populate UOM\n",
    "cleansed_df = populate_UOM(cleansed_df)\n",
    "\n",
    "# for testing, and perhaps for the long term populate TPP and TFF parts correctly with FAS and Buy\n",
    "cleansed_df = populate_Source_Code(cleansed_df)\n",
    "\n",
    "# in case some of the fields have been populated using the XEN tool in 3dx and some have been missed\n",
    "cleansed_df = populate_mandatory_fields(cleansed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop blank source codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we don't do this for the time-being?  let everything get written out\n",
    "# cleansed_df = drop_missing_source_codes(cleansed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop long part numbers and their assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop assemblies with part numbers greater than 25 chars\n",
    "cleansed_df, long_pn = long_part_numbers(cleansed_df, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMO processing\n",
    "\n",
    "If we are going to CMO cars and not manufacture them ourselves we don't need to Buy assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark.chinnock\\AppData\\Local\\Temp\\ipykernel_32544\\1681251629.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Provide'] = np.where(df['Level'] == 4, 'Phantom', df['Provide'])\n"
     ]
    }
   ],
   "source": [
    "# CMO is setup earlier when setting the company to GMT - wanted to use a separate flag in case gmt doesn't always mean CMO true\n",
    "if CMO:\n",
    "    cleansed_df = handle_CMO(cleansed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create structure and parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_df, parts_df = create_structure_parts(cleansed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate missing values in the top levels\n",
    "We need these rows for the original structure file, so we can't afford to drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case they haven't been populated in 3dx, we can fill level 0-3 with SC=SYS, PROVIDE=Make and INVENTORY_PART_PLANNING=P\n",
    "parts_df['SOURCE_CODE'] = np.where((parts_df['SOURCE_CODE'].isna()) & (parts_df['PART_LEVEL'] < 4), 'SYS', parts_df['SOURCE_CODE'])\n",
    "parts_df['PROVIDE'] = np.where((parts_df['PROVIDE'].isna()) & (parts_df['PART_LEVEL'] < 4), 'Make', parts_df['PROVIDE'])\n",
    "parts_df['INVENTORY_PART_PLANNING'] = np.where((parts_df['INVENTORY_PART_PLANNING'].isna()) & (parts_df['PART_LEVEL'] < 4), 'P', parts_df['INVENTORY_PART_PLANNING'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the issue level for the parent part.  Then map this to the PART_REV column of the structure_df\n",
    "\n",
    "parent_part_dict = {}\t\n",
    "# parent_part_dict = pd.Series(IFS['Issue Level'][IFS['Part Number'].isna()==False].values,index=IFS['Part Number'][IFS['Part Number'].isna()==False]).to_dict()\n",
    "# changed this to use cleansed_df\n",
    "parent_part_dict = pd.Series(cleansed_df['Revision'][cleansed_df['Title'].isna()==False].values,index=cleansed_df['Title'][cleansed_df['Title'].isna()==False]).to_dict()\n",
    "\n",
    "# this creates the PART_REV column\n",
    "structure_df['PART_REV'] = structure_df['PART_NO'].map(parent_part_dict)\n",
    "\n",
    "# now correct the column ordering for the final file template\n",
    "structure_df = structure_df[['PART_NO', 'PART_REV', 'SUB_PART_NO', 'QTY', 'SUB_PART_REV']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PART_NO</th>\n",
       "      <th>PART_LEVEL</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>WEIGHT_NET</th>\n",
       "      <th>SOURCE_CODE</th>\n",
       "      <th>Revision</th>\n",
       "      <th>FUNCTION_GROUP</th>\n",
       "      <th>SUB_GROUP</th>\n",
       "      <th>PART_RESPONSIBLE</th>\n",
       "      <th>RELEASE_STATUS</th>\n",
       "      <th>UNIT_CODE</th>\n",
       "      <th>PROVIDE</th>\n",
       "      <th>INVENTORY_PART_PLANNING</th>\n",
       "      <th>MATURITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PART_NO, PART_LEVEL, DESCRIPTION, WEIGHT_NET, SOURCE_CODE, Revision, FUNCTION_GROUP, SUB_GROUP, PART_RESPONSIBLE, RELEASE_STATUS, UNIT_CODE, PROVIDE, INVENTORY_PART_PLANNING, MATURITY]\n",
       "Index: []"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_df[structure_df['PART_NO'] == 'T48E-A02191']\n",
    "parts_df[parts_df['PART_NO'] == 'T48E-A02191']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df['INFO_TEXT'] = 'Drawing URL?'\n",
    "# map LOT_TRACKING_CODE from existing parts, 22/06/2023 - changed to default to Not Lot Tracking.  This is usual default\n",
    "parts_df['LOT_TRACKING_CODE'] = parts_df['PART_NO'].map(parts_cat_dict).fillna('Not Lot Tracking')\n",
    "# parts_df['LOT_TRACKING_CODE'] = 'Order Based'\n",
    "parts_df['SERIAL_RULE'] = 'Manual'\n",
    "parts_df['CONFIGURABLE'] = 'Not Configured'\n",
    "parts_df['AQUISITION_CODE'] = np.where(parts_df['PROVIDE'] == 'Phantom', 'No Demand', 'Demand')\n",
    "parts_df['PLANNING_METHOD'] = np.where(company=='gmt','PMRP Planned', 'Standard Planned')\n",
    "# lookup part issue level for ENG_REV_NO.\n",
    "parts_df['ENG_PART_REV'] = parts_df['Revision']\n",
    "# lookup if this is a parent part with a different issue level, otherwise leave as the part issue level\n",
    "# parts_df['ENG_PART_REV'] = parts_df['PART_NO'].map(parent_part_dict).fillna(parts_df['ENG_PART_REV'])\n",
    "# look up existing part information for serial tracking first\n",
    "parts_df['SERIAL_TRACKING_CODE'] = parts_df['PART_NO'].map(ifs_serial_tracking)\n",
    "# and then set all remaining isna() to Not Serial Tracker\n",
    "parts_df['SERIAL_TRACKING_CODE'] = np.where(parts_df['SERIAL_TRACKING_CODE'].isna(), 'Not Serial Tracking', parts_df['SERIAL_TRACKING_CODE'])\n",
    "# map to an existing UNIT_CODE if we have one, otherwise keep it as passed in. \n",
    "parts_df['UNIT_CODE'] = parts_df['PART_NO'].map(ifs_parts_dict).fillna(parts_df['UNIT_CODE'])\n",
    "# part level 0 is exception to the rule above and will always be PROVIDE = 'Make', INVENTORY_PART_PLANNING = 'A'\n",
    "parts_df['INVENTORY_PART_PLANNING'] = np.where(parts_df['PART_LEVEL'] == 0, 'A', parts_df['INVENTORY_PART_PLANNING'])\n",
    "# create default status for inventory part status of 'A' for purchase/purchase raw, and 'I' for make parts\n",
    "parts_df['INVENTORY_PART_STATUS'] = np.where(parts_df['PROVIDE']=='Make', 'A', 'I')\n",
    "\n",
    "# don't need these columns anymore\n",
    "parts_df.drop(columns=['Revision'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default weights to zero where not provided\n",
    "parts_df['WEIGHT_NET'] = np.where(parts_df['WEIGHT_NET'].isna(), 0, parts_df['WEIGHT_NET'])\n",
    "\n",
    "# get rid of negative weights whilst we're waiting for BoM to be corrected\n",
    "parts_df['WEIGHT_NET'] = np.where(parts_df['WEIGHT_NET'] < 0, 0, parts_df['WEIGHT_NET'])\n",
    "\n",
    "# add blank VARIANT but replacing MATURITY with DEVELOPMENT \n",
    "parts_df['VARIANT'] = np.NaN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENG_REV_NO is an IFS internally incremented number.  We don't have control over it so we are not going to pass it.\n",
    "# migration script in IFS will handle this\n",
    "\n",
    "part_cols_ordered = ['PART_NO',\n",
    "'DESCRIPTION',\n",
    "'WEIGHT_NET',\n",
    "'INFO_TEXT',\n",
    "'UNIT_CODE',\n",
    "'LOT_TRACKING_CODE',\n",
    "'SERIAL_RULE',\n",
    "'SERIAL_TRACKING_CODE',\n",
    "'CONFIGURABLE',\n",
    "'PROVIDE',\n",
    "'AQUISITION_CODE',\n",
    "'PLANNING_METHOD',\n",
    "'PART_RESPONSIBLE',\n",
    "'ENG_PART_REV',\n",
    "# 'ENG_REV_NO',\n",
    "'FUNCTION_GROUP',\n",
    "'SUB_GROUP',\n",
    "'PART_LEVEL',\n",
    "'SOURCE_CODE',\n",
    "'VARIANT',\n",
    "'MATURITY',\n",
    "'INVENTORY_PART_PLANNING',\n",
    "'RELEASE_STATUS',\n",
    "'INVENTORY_PART_STATUS'\n",
    "]\n",
    "\n",
    "parts_df = parts_df[part_cols_ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we're not going to default any values from 3dx \n",
    "# # fill blank UNIT_CODE with LTR for FLA\n",
    "# parts_df['UNIT_CODE'] = np.where(((parts_df['UNIT_CODE'].isna()) & (parts_df['SOURCE_CODE'] == 'FLA')), 'LTR', parts_df['UNIT_CODE'])\n",
    "\n",
    "# # fill remaining blank UNIT_CODE with PCS\n",
    "# parts_df['UNIT_CODE'] = parts_df['UNIT_CODE'].fillna('PCS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df['PART_LEVEL'] = parts_df['PART_LEVEL'].astype(int)\n",
    "# parts_df['ENG_PART_REV'] = np.round(parts_df['ENG_PART_REV'], decimals=2)\n",
    "parts_df['WEIGHT_NET'] = np.round(parts_df['WEIGHT_NET'], decimals = 4)\n",
    "# parts_df['WEIGHT_NET'] = parts_df['WEIGHT_NET'].truncate(8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates from parts_df and structure_df\n",
    "\n",
    "We've done all the work and calculated the quantities.  IFS only need to be told about each of the parts once, and told of the structures once.\n",
    "We don't sum the quantities again - we want 1 steering wheel and reference it in 4 places for the options. Just need to tell IFS once\n",
    "\n",
    "If we have this situation, where T50-B0897 and T50-B0020 part and parent are mentioned more than once, we only need to tell IFS once:\n",
    "\n",
    "```   \n",
    "SA_Index           Part Number  Parent Part  Issue Level\n",
    "17481.0_T50-B0198  T50-B0198    T50-B0020    3.0            1\n",
    "17481.0_T50-B0365  T50-B0365    T50-B0020    1.0            1\n",
    "17481.0_T50-B0841  T50-B0841    T50-B0020    1.0            1\n",
    "17481.0_T50-B0843  T50-B0843    T50-B0020    1.0            1\n",
    "17481.0_T50-B0845  T50-B0845    T50-B0020    1.0            1\n",
    "17481.0_T50-B0847  T50-B0847    T50-B0020    1.0            1\n",
    ">>17481.0_T50-B0897  T50-B0897    T50-B0020    1.0            1<<\n",
    "17481.0_TFF-SA800  TFF-SA800    T50-B0020    1.0            1\n",
    "17501.0_T50-B0198  T50-B0198    T50-B0020    3.0            1\n",
    "17501.0_T50-B0365  T50-B0365    T50-B0020    1.0            1\n",
    "17501.0_T50-B0841  T50-B0841    T50-B0020    1.0            1\n",
    "17501.0_T50-B0843  T50-B0843    T50-B0020    1.0            1\n",
    "17501.0_T50-B0845  T50-B0845    T50-B0020    1.0            1\n",
    "17501.0_T50-B0847  T50-B0847    T50-B0020    1.0            1\n",
    ">>17501.0_T50-B0897  T50-B0897    T50-B0020    1.0            1<<\n",
    "17501.0_TFF-AA059  TFF-AA059    T50-B0020    1.0            1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_df = parts_df.drop_duplicates(subset=['PART_NO','ENG_PART_REV'])\n",
    "# structure_df = structure_df.drop_duplicates()\n",
    "# rather that drop duplicates in structures we should merge the parts and sum the qtys\n",
    "# cleansed_df[['Parent Part','Title','Revision','Quantity']][cleansed_df['Title'] == 'T48E-P06323']\n",
    "structure_df = structure_df.groupby(['PART_NO','PART_REV','SUB_PART_NO','SUB_PART_REV']).QTY.sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PART_NO</th>\n",
       "      <th>PART_REV</th>\n",
       "      <th>SUB_PART_NO</th>\n",
       "      <th>SUB_PART_REV</th>\n",
       "      <th>QTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PART_NO, PART_REV, SUB_PART_NO, SUB_PART_REV, QTY]\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these structures are still duplicated\n",
    "structure_df[structure_df.duplicated(subset=['PART_NO','SUB_PART_NO','SUB_PART_REV'], keep=False)].sort_values(by=['PART_NO','SUB_PART_NO'])\n",
    "# structure_df[(structure_df['PART_NO'] == 'T50-A5285') & (structure_df['SUB_PART_NO'] == 'TFF-SA907')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increment ENG_PART_REV\n",
    "\n",
    "If there is a released part already in IFS with the same ENG_PART_REV, and we're changing the structure of the part, IFS will ignore it.  We need to increment this value to make IFS recognise the change.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop trailing zeros when writing out to csv\n",
    "parts_df['ENG_PART_REV'] = parts_df['ENG_PART_REV'].apply(str)\n",
    "structure_df['PART_REV'] = structure_df['PART_REV'].apply(str)\n",
    "structure_df['SUB_PART_REV'] = structure_df['SUB_PART_REV'].apply(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prev_timestamps != 'None':\n",
    "    prev_part, prev_struct = read_prev_mig_files(outdir, prev_timestamps)\n",
    "\n",
    "    # derive incrementer from previous part file\n",
    "    # incrementer = prev_part['ENG_PART_REV'].str.split('.').str[-1].astype(int).unique()\n",
    "    incrementer = prev_part['ENG_PART_REV'].astype(str).str.split('.').str[-1].astype(int).unique()\n",
    "    try:\n",
    "        len(incrementer) == 1\n",
    "    except:\n",
    "        logit.exception(\"More than one migration revision found in previous file\")\n",
    "        raise\n",
    "\n",
    "    incrementer = incrementer[0]\n",
    "    incrementer += 1\n",
    "\n",
    "    delta_parts, delta_struct = build_delta_files(parts_df, structure_df)\n",
    "\n",
    "    changed_parts = set(delta_parts['PART_NO'].tolist())\n",
    "    # get the assembly (SA_Index) for any assembly that has those Part numbers\n",
    "    delta_sa_index = cleansed_df['SA_Index'].str.split('_').str[0][cleansed_df['Part Number'].isin(changed_parts)].tolist()\n",
    "\n",
    "    # set to give us a unique list\n",
    "    sa_set = set(delta_sa_index)\n",
    "\n",
    "    delta_df = pd.DataFrame()\n",
    "    rel_delta_df = pd.DataFrame()\n",
    "    unrel_delta_df = pd.DataFrame()\n",
    "\n",
    "    # create regex pattern for word match at start of string followed by any number of chars\n",
    "    if len(sa_set) > 0:\n",
    "        pat = '|'.join(r\"\\b^{}.*\\b\".format(x) for x in sa_set)\n",
    "        # build the delta_df\n",
    "        delta_df = cleansed_df[cleansed_df['SA_Index'].str.contains(pat)]\n",
    "        # sort it\n",
    "        delta_df = delta_df.sort_values('orig_sort')\n",
    "    else:\n",
    "        logit.warning(\"No changes found for this delta\")\n",
    "        print (\"No changes found for this delta\")\n",
    "\n",
    "else:\n",
    "    # first time in\n",
    "    incrementer = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing not incrementing the PART_REV/SUB_PART_REV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the amount we'll add to the ENG_PART_REV to avoid issues in IFS\n",
    "# make a string version of the incrementor\n",
    "str_incr = '.' + str(incrementer)\n",
    "# parts_df['ENG_PART_REV'] = parts_df['ENG_PART_REV'] + str_incr\n",
    "# structure_df['PART_REV'] = structure_df['PART_REV'] + str_incr\n",
    "# structure_df['SUB_PART_REV'] = structure_df['SUB_PART_REV'] + str_incr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Make without Buys\n",
    "\n",
    "IFS won't handle parents of Make where there are no child parts to buy\n",
    "\n",
    "if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prev_timestamps != 'None':\n",
    "    make_no_buy = check_make_no_buy(delta_df)\n",
    "\n",
    "    # remove any assemblies with make and no buy\n",
    "    make_no_buy_df = pd.DataFrame()\n",
    "    if len(make_no_buy) > 0:\n",
    "        pat = '|'.join(r\"\\b^{}.*\\b\".format(x.split('_')[0]) for x.split('_')[0] in make_no_buy)\n",
    "        rel_sc_delta_df = rel_sc_delta_df[~rel_sc_delta_df['SA_Index'].str.contains(pat)]\n",
    "\n",
    "        # get the make no buy part for writing out the warning messages\n",
    "        make_no_buy_df = delta_df[delta_df['SA_Index'] == x]\n",
    "\n",
    "    # pass all the remaining parts left in the \n",
    "    delta_parts_for_all_sa = rel_sc_delta_df['Part Number'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prev_timestamps != 'None':\n",
    "    \n",
    "    logit.info(\"{} Assemblies have changed\".format(len(sa_set)))\n",
    "    logit.info(\"{} changed assemblies are not completely released so won't be processed\".format(len(unrel_sa_set)))\n",
    "    logit.warning(\"{} blank source codes that will stop the whole assembly being released\".format(len(empty_sc)))\n",
    "    logit.warning(\"{} Make parts without any Buy child parts that will stop the whole assembly being migrated\".format(len(make_no_buy)))\n",
    "    if unrel_delta_df.shape[0] > 0:\n",
    "        for i, row in unrel_delta_df[['Function Group', 'Sub Group', 'Part Number', 'Parent Part', 'Release Status', 'SA_Index']].iterrows():\n",
    "            logit.warning(\"Part of Unreleased assembly and not processed: {} {} {} {} {}\".format(row['Function Group'], row['Sub Group'], row['Part Number'], row['Parent Part'], row['Release Status']))\n",
    "\n",
    "    if make_no_buy_df.shape[0] > 0:\n",
    "        for i, row in make_no_buy_df[['Function Group', 'Sub Group', 'Part Number', 'Parent Part', 'Source Code', 'SA_Index']].iterrows():\n",
    "            logit.warning(\"Make Part with no buy child so assembly not processed: {} {} {} {} {}\".format(row['Function Group'], row['Sub Group'], row['Part Number'], row['Parent Part'], row['Source Code']))\n",
    "            \n",
    "    print (\"{} Assemblies have changed\".format(len(sa_set)))\n",
    "    print (\"{} changed assemblies that are not completely released so won't be migrated\".format(len(unrel_sa_set)))\n",
    "    print (\"{} blank source codes\".format(len(empty_sc)))\n",
    "    print (\"{} Make parts without any Buy child parts\".format(len(make_no_buy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prev_timestamps != 'None':\n",
    "\n",
    "    # delta_all_parts = cleansed_df['Part Number'][cleansed_df['SA_Index'].str.contains('^{}'.format(sa_set))].to_list()\n",
    "    # delta_parts_df = parts_df[parts_df['PART_NO'].isin(delta_parts_for_all_sa)]\n",
    "    delta_structure_df = structure_df[structure_df['PART_NO'].isin(delta_parts_for_all_sa)]\n",
    "\n",
    "    delta_all_parts = set(delta_parts_for_all_sa + delta_structure_df['SUB_PART_NO'].tolist())\n",
    "\n",
    "    delta_parts_df = parts_df[parts_df['PART_NO'].isin(delta_all_parts)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a test file\n",
    "\n",
    "Provide a part number and will build test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: orig_sort, dtype: int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansed_df.orig_sort[cleansed_df['orig_sort'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Title, dtype: object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleansed_df['SA_Index'][cleansed_df['Title'] == test_part]\n",
    "cleansed_df['Title'][cleansed_df['SA_Index'] == '3467']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_part = 'T48E-N00471'\n",
    "# test_sa_index = set(cleansed_df.orig_sort[cleansed_df['Title'] == test_part])\n",
    "# for x in test_sa_index:\n",
    "#     test_parts = pd.DataFrame()\n",
    "#     temp = []\n",
    "#     temp = cleansed_df['Title'][cleansed_df['SA_Index'] == str(x)]\n",
    "#     test_parts = pd.concat([test_parts, temp])\n",
    "\n",
    "# all_test_parts = set(test_parts[0].to_list())\n",
    "\n",
    "# test_structure_df = structure_df[structure_df['PART_NO'].isin(all_test_parts)]\n",
    "\n",
    "# test_parts_df = parts_df[parts_df['PART_NO'].isin(all_test_parts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# there shouldn't be anything left to drop but we have to drop anything that has NA in any of the columns - must be missing data somewhere\n",
    "empty_values = parts_df[parts_df[['SOURCE_CODE','UNIT_CODE','PROVIDE']].isnull().any(axis=1)]\n",
    "# parts_df = parts_df[~parts_df[['SOURCE_CODE','UNIT_CODE','PROVIDE']].isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_CODE</th>\n",
       "      <th>UNIT_CODE</th>\n",
       "      <th>PROVIDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Phantom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SOURCE_CODE UNIT_CODE  PROVIDE\n",
       "285          NaN       PCS  Phantom\n",
       "286          NaN       PCS      Buy\n",
       "287          NaN       PCS      Buy\n",
       "291          NaN       PCS      Buy\n",
       "292          NaN       PCS      Buy\n",
       "...          ...       ...      ...\n",
       "1920         NaN       PCS      Buy\n",
       "1921         NaN       PCS      Buy\n",
       "1922         NaN       PCS      Buy\n",
       "1923         NaN       PCS      Buy\n",
       "1924         NaN       PCS      Buy\n",
       "\n",
       "[433 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_values[['SOURCE_CODE','UNIT_CODE','PROVIDE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_excel(df, outfile):\n",
    "    with pd.ExcelWriter(os.path.join(outdir, outfile), engine=\"openpyxl\") as writer:\n",
    "            df.to_excel(writer, sheet_name = 'Sheet1', index=False)\n",
    "            ws = writer.sheets['Sheet1']\n",
    "            wb = writer.book\n",
    "            excel_formatting.adjust_col_width_from_col(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_csv(df, outfile):\n",
    "    df.to_csv(os.path.join(outdir, outfile),sep='\\t', index=False, encoding='utf-8')\n",
    "    # parts_df.to_csv(os.path.join(outdir, outfile),sep='\\t', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting just the top CAR part to not have any parent\n",
      "                  PART_NO_x SUB_PART_NO\n",
      "0                T53-Z00002         NaN\n",
      "141               T26-A2135         NaN\n",
      "338              T53-P02751         NaN\n",
      "339  COPY (1) OF T53-A00823         NaN\n",
      "340              T53-P05033         NaN\n",
      "345              T53-A01001         NaN\n",
      "696      MULTI-BRANCHABLE10         NaN\n",
      "697           BRACKET IDEA3         NaN\n",
      "698           BRACKET IDEA2         NaN\n",
      "699      MULTI-BRANCHABLE13         NaN\n",
      "700  HPS40-2 BUCHSENSTECKER         NaN\n",
      "\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-A01001 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-P05033 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-P05033 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-P05033 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-P05033 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-Z00002 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-Z00002 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-Z00002 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-Z00002 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-Z00002 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n",
      "missing_parent: T53-Z00002 Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\n"
     ]
    }
   ],
   "source": [
    "# Validation checks before writing out.  Don't write files without an error_count of zero\n",
    "timestr = get_current_time()\n",
    "\n",
    "TEST=False\n",
    "DELTA=False\n",
    "\n",
    "if TEST:\n",
    "    print (\"*** TEST MODE ***\")\n",
    "    out_structure_df = test_structure_df\n",
    "    out_parts_df = test_parts_df\n",
    "else:\n",
    "    out_structure_df = structure_df\n",
    "    out_parts_df = parts_df\n",
    "     \n",
    "\n",
    "error_count = 0\n",
    "\n",
    "# check for zero quantities\n",
    "zero_quantities = out_structure_df[out_structure_df['QTY'] == 0]\n",
    "if zero_quantities.shape[0] > 0:\n",
    "    zero_quantities.to_excel(os.path.join(base, '{}_Zero_Quantity.xlsx'.format(project)))\n",
    "    logit.error(\"Zero Quantities found - needs resolving first!\")\n",
    "    print (\"Zero Quantities found - needs resolving first!\")\n",
    "    error_count += 1\n",
    "\n",
    "# check for decimal quantities\n",
    "decimal_quantities = out_structure_df[(out_structure_df['QTY'] > 0) & (out_structure_df['QTY'] < 1)]\n",
    "if decimal_quantities.shape[0] > 0:\n",
    "    decimal_quantities.to_excel(os.path.join(base, '{}_Decimal_Quantity.xlsx'.format(project)))\n",
    "    logit.error(\"Decimal Quantities found - needs resolving first!\")\n",
    "    print (\"Decimal Quantities found - needs resolving first!\")\n",
    "    error_count += 1\n",
    "\n",
    "# check all parts present\n",
    "orphaned_parts = []\n",
    "orphaned_parts = pd.merge(out_parts_df, out_structure_df, left_on='PART_NO', right_on='SUB_PART_NO', how='left', indicator=True)\n",
    "orphaned_parts = orphaned_parts[['PART_NO_x','SUB_PART_NO']][orphaned_parts['_merge'] == 'left_only']\n",
    "if orphaned_parts.shape[0] > 1:\n",
    "    logit.error(\"Expecting just the top CAR part to not have any parent\")\n",
    "    logit.error(\"orphaned parts {}\".format(orphaned_parts))\n",
    "    print (\"Expecting just the top CAR part to not have any parent\")\n",
    "    print (orphaned_parts)\n",
    "    print (\"\")\n",
    "    error_count =+ 1\n",
    "\n",
    "# check all sub parts in structure file are also in individual parts file\n",
    "no_child_part = []\n",
    "no_child_part = pd.merge(out_structure_df, out_parts_df, left_on='SUB_PART_NO', right_on='PART_NO', how='left', indicator=True)\n",
    "no_child_part = no_child_part[['PART_NO_x','SUB_PART_NO']][no_child_part['_merge'] == 'left_only']\n",
    "if no_child_part.shape[0] > 0:\n",
    "    # not sure this check is true any more\n",
    "    logit.error (\"Not expecting any sub parts in structure file without individual part entry\")\n",
    "    logit.error (no_child_part)\n",
    "    print (\"Not expecting any sub parts in structure file without individual part entry\")\n",
    "    print (no_child_part)\n",
    "    # not worried about errors at the moment as I want a part file on its own\n",
    "    # error_count =+ 1\n",
    "\n",
    "# st\n",
    "sub_part_rev_check = []\n",
    "sub_part_rev_check = pd.merge(out_structure_df, out_parts_df, left_on=['SUB_PART_NO','SUB_PART_REV'], right_on=['PART_NO','ENG_PART_REV'], indicator=True, how='left')\n",
    "sub_part_rev_check = sub_part_rev_check[sub_part_rev_check['_merge'] == 'left_only']\n",
    "if sub_part_rev_check.shape[0] > 0:\n",
    "    for x, row in sub_part_rev_check.iterrows():\n",
    "        logit.error (\"sub_part_rev_check: SUB_PART_NO: {} SUB_PART_REV: {} ENG_PART_REV: {} is missing from structures file\".format(row.iloc[2], row.iloc[4], row.iloc[1]))\n",
    "        print (\"sub_part_rev_check: PART_NO: {} SUB_PART_REV: {} ENG_PART_REV: {} is missing from structures file\".format(row.iloc[2], row.iloc[4], row.iloc[1]))\n",
    "        # print (sub_part_rev_check)\n",
    "        error_count =+ 1        \n",
    "\n",
    "# check part_rev is not blank (will be string 'nan' by now)\n",
    "part_rev_check = out_structure_df[out_structure_df.PART_REV == 'nan']\n",
    "if part_rev_check.shape[0] > 0:\n",
    "    for x, row in part_rev_check.iterrows():\n",
    "        logit.error (\"part_rev_check: missing PART_REV for part {}\".format(row.iloc[0], row.iloc[1], row.iloc[2]))\n",
    "        # logit.error (part_rev_check)\n",
    "        print (\"part_rev_check: missing PART_REV for part {}\".format(row.iloc[0], row.iloc[1], row.iloc[2]))\n",
    "    print (part_rev_check)\n",
    "    error_count =+ 1\n",
    "\n",
    "# find master parts in out_structure_df and check PART_REV and SUB_PART_REV match - ignore top row\n",
    "master_part = out_structure_df[['PART_REV','PART_NO']]\n",
    "missing_master = pd.merge(master_part, out_structure_df, left_on=['PART_REV','PART_NO'], right_on=['SUB_PART_REV','SUB_PART_NO'], how='left', indicator=True)\n",
    "missing_master = missing_master[missing_master['_merge'] == 'left_only']\n",
    "# top part to ignore as won't be a match\n",
    "top_part = out_structure_df.iloc[0]['PART_NO']\n",
    "missing_master = missing_master[~missing_master['PART_NO_x'].str.contains(top_part)]\n",
    "if TEST:\n",
    "    # for a test file we won't need the structure for the actual assembly we've specified\n",
    "    missing_master = missing_master[~missing_master['PART_NO_x'].str.contains(test_part)]\n",
    "\n",
    "if missing_master.shape[0] > 0:\n",
    "    for x, row in missing_master.iterrows():\n",
    "        logit.error (\"missing_parent: {} Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\".format(row.iloc[1]))\n",
    "        # logit.error (missing_master)\n",
    "        print (\"missing_parent: {} Must find PART_REV / PART_NO combo in SUB_PART_REV / SUB_PART_NO\".format(row.iloc[1]))\n",
    "    # print (missing_master)\n",
    "    # not checking this at the moment - just want a file\n",
    "    # error_count =+ 1   \n",
    "\n",
    "long_pn = parts_df[parts_df['PART_NO'].str.len() > 25]\n",
    "if long_pn.shape[0] > 0:\n",
    "    print (\"PART_NO is longer than 25 chars\")\n",
    "    print (long_pn['PART_NO'])\n",
    "    error_count =+ 1\n",
    "\n",
    "\n",
    "# all the errors relate to structure file validation.  Parts file can be produced\n",
    "\n",
    "outfile_part = 'P_{}_{}_{}_{}'.format(project, env, incrementer, timestr)\n",
    "outfile_structure = 'S_{}_{}_{}_{}'.format(project, env, incrementer, timestr)\n",
    "\n",
    "if TEST:\n",
    "    outfile_part = outfile_part + '_' + test_part\n",
    "    if error_count == 0: \n",
    "        outfile_structure = outfile_structure + '_' + test_part\n",
    "\n",
    "if DELTA:\n",
    "    logit.info(\"*** Writing out changes since {} file ***\".format(prev_timestamps))\n",
    "    print (\"*** Writing out changes since {} file ***\".format(prev_timestamps))\n",
    "    write_to_excel(delta_parts_df, 'DELTA_{}.xlsx'.format(outfile_part))\n",
    "    write_to_csv(delta_parts_df, 'DELTA_{}.txt'.format(outfile_part))\n",
    "    if error_count == 0: \n",
    "        write_to_excel(delta_structure_df, 'DELTA_{}.xlsx'.format(outfile_structure))\n",
    "        write_to_csv(delta_structure_df, 'DELTA_{}.txt'.format(outfile_structure))\n",
    "\n",
    "    # write out the highlighted changes\n",
    "    compare_file = 'COMPARE_{}_vs_{}.xlsx'.format(prev_timestamps, timestr)\n",
    "    compare_out = os.path.join(outdir, compare_file)\n",
    "    df_final_parts = compare_parts(delta_parts)\n",
    "    try:\n",
    "        delta_struct.loc[1,]\n",
    "        try:\n",
    "            delta_struct.loc[2,]\n",
    "            df_final_struct = compare_struct(delta_struct)\n",
    "        except (KeyError):\n",
    "            print (\"No current parts - assumed all changes are parts being removed\")\n",
    "            # nothing to compare so just provide an empty delta_struct\n",
    "            df_final_struct = delta_struct\n",
    "    except (KeyError) as e:\n",
    "        print (\"No previous parts - assumed all changes are parts being added\")\n",
    "        df_final_struct = delta_struct\n",
    "    # df_final_part.style.apply(highlight_diff, axis=None).to_excel(compare_out, engine='openpyxl',)\n",
    "    with pd.ExcelWriter(compare_out) as writer:\n",
    "        try:\n",
    "            df_final_struct.loc[1,]\n",
    "            try:\n",
    "                df_final_struct.loc[2,]\n",
    "                # highlight the differences between index 1 and 2\n",
    "                df_final_struct.style.apply(highlight_diff, axis=None).to_excel(writer, sheet_name='Structure')\n",
    "            except (KeyError) as e:\n",
    "                # nothing in 2 to compare.  Write out the structures involved with the parts\n",
    "                delta_structure_df.to_excel(writer, sheet_name='Structure')\n",
    "        except (KeyError) as e:\n",
    "            # nothing in 1 to compare.  Write out the structures involved with the parts\n",
    "            delta_structure_df.to_excel(writer, sheet_name='Structure')\n",
    "\n",
    "        df_final_parts.style.apply(highlight_diff, axis=None).to_excel(writer, sheet_name='Parts')\n",
    "\n",
    "    # write out the delta df file for Lorena to file a bom-like file\n",
    "    delta_bom_file = 'Changed_Assemblies_BOM_{}_{}_vs_{}.xlsx'.format(project, prev_timestamps, timestr)\n",
    "    delta_bom_out = os.path.join(outdir, delta_bom_file)\n",
    "    write_to_excel(delta_df, delta_bom_out)\n",
    "\n",
    "# parts files can be written out - error checks are structure file only\n",
    "write_to_excel(out_parts_df, outfile_part + '.xlsx')\n",
    "write_to_csv(out_parts_df, outfile_part + '.txt')\n",
    "if error_count == 0: \n",
    "    write_to_excel(out_structure_df, outfile_structure + '.xlsx')\n",
    "    write_to_csv(out_structure_df, outfile_structure + '.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PART_NO</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>WEIGHT_NET</th>\n",
       "      <th>INFO_TEXT</th>\n",
       "      <th>UNIT_CODE</th>\n",
       "      <th>LOT_TRACKING_CODE</th>\n",
       "      <th>SERIAL_RULE</th>\n",
       "      <th>SERIAL_TRACKING_CODE</th>\n",
       "      <th>CONFIGURABLE</th>\n",
       "      <th>PROVIDE</th>\n",
       "      <th>...</th>\n",
       "      <th>ENG_PART_REV</th>\n",
       "      <th>FUNCTION_GROUP</th>\n",
       "      <th>SUB_GROUP</th>\n",
       "      <th>PART_LEVEL</th>\n",
       "      <th>SOURCE_CODE</th>\n",
       "      <th>VARIANT</th>\n",
       "      <th>MATURITY</th>\n",
       "      <th>INVENTORY_PART_PLANNING</th>\n",
       "      <th>RELEASE_STATUS</th>\n",
       "      <th>INVENTORY_PART_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PART_NO, DESCRIPTION, WEIGHT_NET, INFO_TEXT, UNIT_CODE, LOT_TRACKING_CODE, SERIAL_RULE, SERIAL_TRACKING_CODE, CONFIGURABLE, PROVIDE, AQUISITION_CODE, PLANNING_METHOD, PART_RESPONSIBLE, ENG_PART_REV, FUNCTION_GROUP, SUB_GROUP, PART_LEVEL, SOURCE_CODE, VARIANT, MATURITY, INVENTORY_PART_PLANNING, RELEASE_STATUS, INVENTORY_PART_STATUS]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# existing_bom[existing_bom['Title'] == 'T48e-P06323']\n",
    "out_parts_df[out_parts_df['PART_NO'] == 'T48E-P06323']\n",
    "# out_structure_df[out_structure_df['SUB_PART_NO'] == 'T48E-P06323']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_sort</th>\n",
       "      <th>BOM COUNT</th>\n",
       "      <th>Matching Key</th>\n",
       "      <th>Last Export Date</th>\n",
       "      <th>Function Group</th>\n",
       "      <th>System</th>\n",
       "      <th>Sub System</th>\n",
       "      <th>Level</th>\n",
       "      <th>Title</th>\n",
       "      <th>Parent Part</th>\n",
       "      <th>...</th>\n",
       "      <th>Target Mass</th>\n",
       "      <th>Type</th>\n",
       "      <th>Variant/Option</th>\n",
       "      <th>extr_function</th>\n",
       "      <th>extr_invalid_code</th>\n",
       "      <th>extr_maturity</th>\n",
       "      <th>extr_pn</th>\n",
       "      <th>extr_project</th>\n",
       "      <th>has_child</th>\n",
       "      <th>part_number_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_sort, BOM COUNT, Matching Key, Last Export Date, Function Group, System, Sub System, Level, Title, Parent Part, Revision, Description, Quantity, Source Code, UOM, Provide, Actual Mass, CAD Mass, CAD Material, Programme Maturity, Subtype, CAD Master, CAD Maturity, CAD Surface Treatment, CAD Version, COG, COG X, COG Y, COG Z, Collaborative Space, Configuration Context, Created From, Creation Date, Current Evolution, Effectivity, Effectivity Type , Estimated Mass, Function System Combined, Instance Title, Instantiated Configuration, Inventory Part Planning, Last Modified By, Legacy ID, Legacy Revision, Lock, Locked By, Major Revision, Manufacturing Process, Manufacturing Process Combined, Manufacturing Process Detailed, Maturity State, Modification Date, Name, Originator, Owner, Part Identification, Part Type, Phase, Position Matrix, Programme Maturity Code, Project, Projected Evolution, Revision Comment, SA_Index, Source Code Name, Surface Maturity Level, Symmetry, Target Mass, Type, Variant/Option, extr_function, extr_invalid_code, extr_maturity, extr_pn, extr_project, has_child, part_number_length]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 77 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(out_structure_df, out_parts_df, left_on=['SUB_PART_NO','SUB_PART_REV'], right_on=['PART_NO','ENG_PART_REV'], indicator=True, how='left')\n",
    "# sub_part_rev_check\n",
    "# out_parts_df.filter(regex='PART|REV')\n",
    "out_structure_df[out_structure_df['SUB_PART_NO'] == 'T48E-P06323']\n",
    "existing_bom[existing_bom['Title'] == 'T48e-P06323']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [PART_NO, PART_REV, SUB_PART_NO, SUB_PART_REV, QTY]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [PART_NO, DESCRIPTION, WEIGHT_NET, INFO_TEXT, UNIT_CODE, LOT_TRACKING_CODE, SERIAL_RULE, SERIAL_TRACKING_CODE, CONFIGURABLE, PROVIDE, AQUISITION_CODE, PLANNING_METHOD, PART_RESPONSIBLE, ENG_PART_REV, FUNCTION_GROUP, SUB_GROUP, PART_LEVEL, SOURCE_CODE, VARIANT, MATURITY, INVENTORY_PART_PLANNING, RELEASE_STATUS, INVENTORY_PART_STATUS]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source Code</th>\n",
       "      <th>UOM</th>\n",
       "      <th>Provide</th>\n",
       "      <th>Inventory Part Planning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Source Code, UOM, Provide, Inventory Part Planning]\n",
       "Index: []"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (structure_df[structure_df['PART_NO'] == 'T48E-A02195'])\n",
    "print (parts_df[parts_df['PART_NO'] == 'T48e-A02195'])\n",
    "existing_bom[['Title','Source Code','UOM','Provide','Inventory Part Planning']][existing_bom['Title'] == 'T48e-A02195']\n",
    "cleansed_df[['Title','Source Code','UOM','Provide','Inventory Part Planning']][cleansed_df['Title'] == 'T48e-A02195']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source Code</th>\n",
       "      <th>Inventory Part Planning</th>\n",
       "      <th>Part Type</th>\n",
       "      <th>Parent Part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Source Code, Inventory Part Planning, Part Type, Parent Part]\n",
       "Index: []"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.merge(structure_df, parts_df, left_on='SUB_PART_NO', right_on='PART_NO', how='left', indicator=True)\n",
    "cleansed_df.filter(regex='Title|Source|Part')[cleansed_df['Parent Part'] == 'T48E-A02773X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PART_NO</th>\n",
       "      <th>PART_REV</th>\n",
       "      <th>SUB_PART_NO</th>\n",
       "      <th>SUB_PART_REV</th>\n",
       "      <th>QTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PART_NO, PART_REV, SUB_PART_NO, SUB_PART_REV, QTY]\n",
       "Index: []"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_rev_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source Code</th>\n",
       "      <th>Instance Title</th>\n",
       "      <th>SA_Index</th>\n",
       "      <th>Source Code Name</th>\n",
       "      <th>Surface Maturity Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>8.0</td>\n",
       "      <td>T53-A01005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T53-A01005.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>9.0</td>\n",
       "      <td>T53-A00905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T53-A00905.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10.0</td>\n",
       "      <td>T53-A00879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T53-A00879.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10.0</td>\n",
       "      <td>T53-A01463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T53-A01439.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>11.0</td>\n",
       "      <td>T53-A01439</td>\n",
       "      <td>BOF</td>\n",
       "      <td>T53-A01439.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>BOF - Bought Out Finished</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>11.0</td>\n",
       "      <td>TFF-SK202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TFF-SK202.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>10.0</td>\n",
       "      <td>T53-A01129</td>\n",
       "      <td>BOF</td>\n",
       "      <td>T53-A01129.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>10.0</td>\n",
       "      <td>T53-A01131</td>\n",
       "      <td>BOF</td>\n",
       "      <td>T53-A01131.1</td>\n",
       "      <td>3_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Level       Title Source Code Instance Title SA_Index  \\\n",
       "50    8.0  T53-A01005         NaN   T53-A01005.1      3_4   \n",
       "51    9.0  T53-A00905         NaN   T53-A00905.1      3_4   \n",
       "52   10.0  T53-A00879         NaN   T53-A00879.1      3_4   \n",
       "53   10.0  T53-A01463         NaN   T53-A01439.1      3_4   \n",
       "54   11.0  T53-A01439         BOF   T53-A01439.1      3_4   \n",
       "55   11.0   TFF-SK202         NaN    TFF-SK202.1      3_4   \n",
       "56   10.0  T53-A01129         BOF   T53-A01129.1      3_4   \n",
       "57   10.0  T53-A01131         BOF   T53-A01131.1      3_4   \n",
       "\n",
       "             Source Code Name  Surface Maturity Level  \n",
       "50                        NaN                     NaN  \n",
       "51                        NaN                     NaN  \n",
       "52                        NaN                     NaN  \n",
       "53                        NaN                     NaN  \n",
       "54  BOF - Bought Out Finished                     NaN  \n",
       "55                        NaN                     NaN  \n",
       "56                        NaN                     NaN  \n",
       "57                        NaN                     NaN  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_bom.filter(regex='Title|Level|Source|SA_Index').loc[50:57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.info('Completed')\n",
    "\n",
    "for handler in logit.handlers:\n",
    "    if isinstance(handler, logging.FileHandler):\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logit.handlers:\n",
    "    if isinstance(handler, logging.FileHandler):\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
